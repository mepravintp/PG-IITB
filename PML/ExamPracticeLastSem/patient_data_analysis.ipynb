{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac575599",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in is_diabetic column: 0\n",
      "Imbalance in is_diabetic column:\n",
      "is_diabetic\n",
      "0    481\n",
      "1    260\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Display the first few rows of the DataFrame\n",
    "df = pd.read_csv('health_data.csv')\n",
    "\n",
    "# is_diabetic column is my Y variable\n",
    "# find missing values for is_diabetic column\n",
    "\n",
    "missing_values = df['is_diabetic'].isnull().sum()\n",
    "print(f'Missing values in is_diabetic column: {missing_values}')\n",
    "\n",
    "#find imbalance in is_diabetic column\n",
    "imbalance = df['is_diabetic'].value_counts()\n",
    "print('Imbalance in is_diabetic column:')\n",
    "print(imbalance)\n",
    "\n",
    "# find imbalance \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e543d52e",
   "metadata": {},
   "source": [
    "### Class Imbalance in is_diabetic Column\n",
    "\n",
    "The target variable `is_diabetic` is imbalanced:\n",
    "- 0 (non-diabetic): 481 samples\n",
    "- 1 (diabetic): 260 samples\n",
    "\n",
    "This means there are significantly more non-diabetic cases than diabetic cases. Class imbalance can affect model performance, especially for classification tasks, as models may be biased towards the majority class. Consider using techniques such as resampling, class weights, or appropriate metrics (like precision, recall, F1-score) to address imbalance when building predictive models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5e35811e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.74      0.80      0.76        98\n",
      "           1       0.53      0.45      0.49        51\n",
      "\n",
      "    accuracy                           0.68       149\n",
      "   macro avg       0.64      0.62      0.63       149\n",
      "weighted avg       0.67      0.68      0.67       149\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluate model with as is data without any imputation or balancing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "# Split the data into features and target variable\n",
    "X = df.drop('is_diabetic', axis=1)\n",
    "y = df['is_diabetic']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "model = RandomForestClassifier(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))# in is_diabetic column"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e119eafa",
   "metadata": {},
   "source": [
    "### Model Evaluation: Classification Report Interpretation\n",
    "\n",
    "- **Class 0 (non-diabetic):**\n",
    "  - Precision: 0.74\n",
    "  - Recall: 0.80\n",
    "  - F1-score: 0.76\n",
    "  - Support: 98\n",
    "\n",
    "- **Class 1 (diabetic):**\n",
    "  - Precision: 0.53\n",
    "  - Recall: 0.45\n",
    "  - F1-score: 0.49\n",
    "  - Support: 51\n",
    "\n",
    "**Interpretation:**\n",
    "- The model performs better for the majority class (non-diabetic) than for the minority class (diabetic).\n",
    "- Lower recall and F1-score for the diabetic class indicate that many diabetic cases are missed (false negatives).\n",
    "- This is a common issue with imbalanced datasets, where the model is biased towards the majority class.\n",
    "- To improve performance for the minority class, consider techniques such as resampling (oversampling/undersampling), using class weights, or trying different algorithms and evaluation metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c7d44b26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      0.81      0.82       105\n",
      "           1       0.78      0.80      0.79        88\n",
      "\n",
      "    accuracy                           0.80       193\n",
      "   macro avg       0.80      0.80      0.80       193\n",
      "weighted avg       0.80      0.80      0.80       193\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# evaulate model with imputation and balancing\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from imblearn.over_sampling import SMOTE\n",
    "# Impute missing values\n",
    "imputer = SimpleImputer(strategy='mean')\n",
    "X_imputed = imputer.fit_transform(X)\n",
    "# Balance the dataset using SMOTE\n",
    "smote = SMOTE(random_state=30)  \n",
    "X_balanced, y_balanced = smote.fit_resample(X_imputed, y)   \n",
    "# Split the balanced data\n",
    "X_train_bal, X_test_bal, y_train_bal, y_test_bal = train_test_split(X_balanced, y_balanced, test_size=0.2, random_state=42)\n",
    "# Train the model\n",
    "model_bal = RandomForestClassifier(random_state=30)\n",
    "model_bal.fit(X_train_bal, y_train_bal)\n",
    "y_pred_bal = model_bal.predict(X_test_bal)\n",
    "print(classification_report(y_test_bal, y_pred_bal))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4508dfde",
   "metadata": {},
   "source": [
    "### Model Evaluation After Imputation and Balancing (SMOTE)\n",
    "\n",
    "- **Class 0 (non-diabetic):**\n",
    "  - Precision: 0.83\n",
    "  - Recall: 0.81\n",
    "  - F1-score: 0.82\n",
    "  - Support: 105\n",
    "\n",
    "- **Class 1 (diabetic):**\n",
    "  - Precision: 0.78\n",
    "  - Recall: 0.80\n",
    "  - F1-score: 0.79\n",
    "  - Support: 88\n",
    "\n",
    "**Interpretation:**\n",
    "- After imputing missing values and balancing the dataset with SMOTE, the model's performance for both classes has improved and is now more balanced.\n",
    "- Precision, recall, and F1-score for the minority class (diabetic) are much higher compared to the previous model.\n",
    "- This demonstrates the effectiveness of handling missing data and class imbalance for improving classification results.\n",
    "- Both classes are now predicted with similar accuracy, reducing bias towards the majority class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "413372a3",
   "metadata": {},
   "source": [
    "### General Rule for Identifying Class Imbalance\n",
    "\n",
    "Class imbalance occurs when the number of samples in one class is much higher or lower than in other classes. \n",
    "\n",
    "**General Rule:**\n",
    "- If the ratio of the minority class to the majority class is less than 1:2 (or 33%), the dataset is considered imbalanced.\n",
    "- Severe imbalance is often defined as a minority class ratio below 10%.\n",
    "\n",
    "**How to Check:**\n",
    "- Use `value_counts()` on the target variable to see the distribution of classes.\n",
    "- Calculate the percentage of each class:\n",
    "  ```python\n",
    "  class_distribution = y.value_counts(normalize=True)\n",
    "  print(class_distribution)\n",
    "  ```\n",
    "- If one class is much less frequent, consider the dataset imbalanced and apply appropriate techniques to address it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8243104f",
   "metadata": {},
   "source": [
    "### How to Deal with Class Imbalance\n",
    "\n",
    "Common techniques to address class imbalance:\n",
    "\n",
    "1. **Resampling Methods**\n",
    "   - **Oversampling**: Increase the number of minority class samples (e.g., SMOTE).\n",
    "   - **Undersampling**: Reduce the number of majority class samples.\n",
    "\n",
    "2. **Class Weights**\n",
    "   - Assign higher weights to the minority class in model training (supported by many algorithms).\n",
    "\n",
    "3. **Ensemble Methods**\n",
    "   - Use ensemble models (e.g., Random Forest, XGBoost) that handle imbalance better.\n",
    "\n",
    "4. **Algorithm Selection**\n",
    "   - Choose algorithms robust to imbalance or specifically designed for imbalanced data.\n",
    "\n",
    "5. **Evaluation Metrics**\n",
    "   - Use metrics like precision, recall, F1-score, ROC-AUC instead of accuracy.\n",
    "\n",
    "6. **Data Augmentation**\n",
    "   - Generate synthetic samples for the minority class.\n",
    "\n",
    "7. **Threshold Tuning**\n",
    "   - Adjust decision thresholds to improve minority class detection.\n",
    "\n",
    "Select and combine these techniques based on your data and problem context for best results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea76983",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
